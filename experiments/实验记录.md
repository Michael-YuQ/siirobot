# AT-PC 实验记录

## 第一轮实验 (2026-02-09 ~ 02-10)

### 基本信息

- 平台: Google Colab (T4 GPU)
- 配置: 3 方法 (DR / PAIRED / AT-PC) × 4 生成器 (G1-G4) × 3 种子 = 36 runs
- 每 run 2000 iterations, 约 16-28 分钟
- 36 个实验全部完成，无运行失败

### 已知问题

#### 1. reward 记录全为 0 (日志 bug)

`training_stats.json` 中 `reward` 字段全部为 0。

原因: 代码中尝试读取 `rew_tracking_lin_vel` 这个 key，但环境实际不存在该字段，导致 fallback 到 0。

影响: 无法从日志中看到真实的 reward 训练曲线。`solver_reward` 字段正常记录（从 env 的 value function 取），可作为替代参考。

示例 (atpc-G1-seed42):
```
iter=0:   reward=0, solver_reward=0.0009
iter=100: reward=0, solver_reward=0.0054
iter=500: reward=0, solver_reward=0.0098
iter=2000: reward=0, solver_reward=0.0128
```

#### 2. 评估脚本 bug

评估脚本存在问题，部分评估结果不可靠。具体 bug 未详细记录，但已在第二轮修复。

#### 3. Accept Rate 极低 (~1%)

新颖性过滤 (novelty filter) 拒绝了约 99% 的生成地形，导致生成器几乎没有有效训练。

原因: 生成器没有 LSTM，无法记忆历史生成的地形，每次生成都像随机采样，很难通过新颖性检测。

影响: AT-PC 和 PAIRED 的效果几乎没有差异（AT-PC 0.0128 vs PAIRED 0.0128），因为 AT-PC 的核心机制（新颖性引导）基本没生效。

#### 4. 三种方法差异不显著

| 方法 | solver_reward (iter 2000, 均值) |
|------|-------------------------------|
| DR   | ~0.013                        |
| PAIRED | ~0.013                      |
| AT-PC | ~0.013                       |

三种方法的 solver_reward 几乎一致，无法体现 AT-PC 的优势。

### 模型文件状态

模型文件本身有效，可以加载和评估。问题仅在于日志记录和评估脚本。

### 数据保存位置

- 网盘服务器: `111.170.6.103:10005/atpc_experiments/`（保留，文件名带时间戳不冲突）
- 本地下载: `dognew/downloaded_results/`

---

## 第二轮实验 (2026-02-10 ~ 进行中)

### 修复内容

1. **reward 记录修复**: 直接从 `env.rew_buf` 取值，新增 `step_reward` 字段
2. **生成器加回 LSTM**: 使生成器能记忆历史，提升新颖性过滤通过率
3. **DR 也做评估**: 第一轮只评估了 PAIRED 和 AT-PC，不公平；这轮三种方法都评估
4. **PAIRED 频率改为 20**: 与原始论文一致（第一轮可能不一致）

### 当前观察 (进行中)

从 paired-G1-seed123 和 paired-G1-seed456 的日志看:

- `rew` 字段有实际值（0.009 ~ 0.783），reward 记录修复确认生效
- Accept Rate = 100%（generator 阶段），远高于第一轮的 ~1%，LSTM 修复有效
- Regret 为小负值（-0.001 ~ -0.003），protagonist 略优于 antagonist，正常
- Reward 波动较大，暂无明显上升趋势

### 待观察

- `step_reward` 是否有上升趋势（判断策略是否在学习）
- 三种方法是否出现分化
- 如果 step_reward 涨但 solver_reward 不涨 → 课程切换太频繁
- 如果两个都不涨 → 可能需要增加迭代数 / 调学习率 / 简化地形

### 潜在风险

solver_reward 在第一轮只到 0.013 量级，说明策略只学会了非常基础的动作。2000 iter 在固定地形上可能够用，但每 10 iter 换一次地形（curriculum_update_freq=10），策略可能还没适应就被换走了。如果第二轮结果仍然不理想，考虑:

- 增加总迭代数（4000 或更多）
- 降低课程更新频率（如 curriculum_update_freq=20 或 50）
- 降低地形难度上限，让策略先学会简单的再逐步加难

---

## 第二轮实验结果 (2026-02-10 ~ 02-11)

### 完成状态

36/36 全部完成。

### 问题确认

1. reward 字段仍为 0 — 修复没生效，原因是 `env.extras["episode"]` 里的值是 Tensor 不是 float，`isinstance(val, (int, float))` 判断失败
2. AT-PC Accept Rate 仍为 ~0.9% — LSTM 修复没有改善新颖性过滤，novelty_threshold=0.7 太严
3. DR 的 stats 没有 solver_reward 字段（只每 50 iter 评估一次，且没写入 stats）
4. 三种方法 solver_reward 仍无分化（均 ~0.012）
5. solver_reward ~0.012 说明策略只学到约 1.2% 的最大可能奖励，基本没学会走路

### 根因分析

- warmup 太短（50 iter = 仅 5 次地形切换），机器狗没学会走就被扔到难地形
- curriculum_update_freq=10 太频繁，策略没时间适应当前地形
- min_easy_prob=0.05 太低，没有保留足够简单地形防止遗忘
- novelty_threshold=0.7 太严，99% 地形被拒绝

---

## 第三轮实验 (2026-02-12 ~ 进行中)

### 修复内容

config.py 改动:
- `warmup_iterations`: 50 → 500（让机器狗先学会走路）
- `curriculum_update_freq`: 10 → 50（给策略更多时间适应当前地形）
- `novelty_threshold`: 0.7 → 0.95（放宽新颖性过滤）
- `easy_terrain_decay`: 0.995 → 0.998（衰减更慢）
- `min_easy_prob`: 0.05 → 0.25（始终保留 25% 简单地形防遗忘）

run_experiment.py 改动:
- reward 记录修复：正确处理 Tensor 类型的 episode reward
- DR 评估频率从 50 iter 改为 10 iter，确保有 solver_reward 数据
- 三种方法统一使用 curriculum_freq=50

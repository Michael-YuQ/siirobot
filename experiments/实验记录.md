# AT-PC 实验记录

## 第一轮实验 (2026-02-09 ~ 02-10)

### 基本信息

- 平台: Google Colab (T4 GPU)
- 配置: 3 方法 (DR / PAIRED / AT-PC) × 4 生成器 (G1-G4) × 3 种子 = 36 runs
- 每 run 2000 iterations, 约 16-28 分钟
- 36 个实验全部完成，无运行失败

### 已知问题

#### 1. reward 记录全为 0 (日志 bug)

`training_stats.json` 中 `reward` 字段全部为 0。

原因: 代码中尝试读取 `rew_tracking_lin_vel` 这个 key，但环境实际不存在该字段，导致 fallback 到 0。

影响: 无法从日志中看到真实的 reward 训练曲线。`solver_reward` 字段正常记录（从 env 的 value function 取），可作为替代参考。

示例 (atpc-G1-seed42):
```
iter=0:   reward=0, solver_reward=0.0009
iter=100: reward=0, solver_reward=0.0054
iter=500: reward=0, solver_reward=0.0098
iter=2000: reward=0, solver_reward=0.0128
```

#### 2. 评估脚本 bug

评估脚本存在问题，部分评估结果不可靠。具体 bug 未详细记录，但已在第二轮修复。

#### 3. Accept Rate 极低 (~1%)

新颖性过滤 (novelty filter) 拒绝了约 99% 的生成地形，导致生成器几乎没有有效训练。

原因: 生成器没有 LSTM，无法记忆历史生成的地形，每次生成都像随机采样，很难通过新颖性检测。

影响: AT-PC 和 PAIRED 的效果几乎没有差异（AT-PC 0.0128 vs PAIRED 0.0128），因为 AT-PC 的核心机制（新颖性引导）基本没生效。

#### 4. 三种方法差异不显著

| 方法 | solver_reward (iter 2000, 均值) |
|------|-------------------------------|
| DR   | ~0.013                        |
| PAIRED | ~0.013                      |
| AT-PC | ~0.013                       |

三种方法的 solver_reward 几乎一致，无法体现 AT-PC 的优势。

### 模型文件状态

模型文件本身有效，可以加载和评估。问题仅在于日志记录和评估脚本。

### 数据保存位置

- 网盘服务器: `111.170.6.103:10005/atpc_experiments/`（保留，文件名带时间戳不冲突）
- 本地下载: `dognew/downloaded_results/`

---

## 第二轮实验 (2026-02-10 ~ 进行中)

### 修复内容

1. **reward 记录修复**: 直接从 `env.rew_buf` 取值，新增 `step_reward` 字段
2. **生成器加回 LSTM**: 使生成器能记忆历史，提升新颖性过滤通过率
3. **DR 也做评估**: 第一轮只评估了 PAIRED 和 AT-PC，不公平；这轮三种方法都评估
4. **PAIRED 频率改为 20**: 与原始论文一致（第一轮可能不一致）

### 当前观察 (进行中)

从 paired-G1-seed123 和 paired-G1-seed456 的日志看:

- `rew` 字段有实际值（0.009 ~ 0.783），reward 记录修复确认生效
- Accept Rate = 100%（generator 阶段），远高于第一轮的 ~1%，LSTM 修复有效
- Regret 为小负值（-0.001 ~ -0.003），protagonist 略优于 antagonist，正常
- Reward 波动较大，暂无明显上升趋势

### 待观察

- `step_reward` 是否有上升趋势（判断策略是否在学习）
- 三种方法是否出现分化
- 如果 step_reward 涨但 solver_reward 不涨 → 课程切换太频繁
- 如果两个都不涨 → 可能需要增加迭代数 / 调学习率 / 简化地形

### 潜在风险

solver_reward 在第一轮只到 0.013 量级，说明策略只学会了非常基础的动作。2000 iter 在固定地形上可能够用，但每 10 iter 换一次地形（curriculum_update_freq=10），策略可能还没适应就被换走了。如果第二轮结果仍然不理想，考虑:

- 增加总迭代数（4000 或更多）
- 降低课程更新频率（如 curriculum_update_freq=20 或 50）
- 降低地形难度上限，让策略先学会简单的再逐步加难

---

## 第二轮实验结果 (2026-02-10 ~ 02-11)

### 完成状态

36/36 全部完成。

### 问题确认

1. reward 字段仍为 0 — 修复没生效，原因是 `env.extras["episode"]` 里的值是 Tensor 不是 float，`isinstance(val, (int, float))` 判断失败
2. AT-PC Accept Rate 仍为 ~0.9% — LSTM 修复没有改善新颖性过滤，novelty_threshold=0.7 太严
3. DR 的 stats 没有 solver_reward 字段（只每 50 iter 评估一次，且没写入 stats）
4. 三种方法 solver_reward 仍无分化（均 ~0.012）
5. solver_reward ~0.012 说明策略只学到约 1.2% 的最大可能奖励，基本没学会走路

### 根因分析

- warmup 太短（50 iter = 仅 5 次地形切换），机器狗没学会走就被扔到难地形
- curriculum_update_freq=10 太频繁，策略没时间适应当前地形
- min_easy_prob=0.05 太低，没有保留足够简单地形防止遗忘
- novelty_threshold=0.7 太严，99% 地形被拒绝

---

## 第三~七轮实验 (2026-02-11 ~ 02-12)

### 问题

训练服务器上 `run_experiment.py` 从未被更新。`git pull` 因本地文件冲突静默失败，导致 config.py 更新了但 run_experiment.py 一直是旧版。

结果：
- step_reward 字段不存在
- reward 仍为 0
- DR 没有 solver_reward
- 三种方法 solver_reward 仍无分化（均 ~0.013）

### 数据

共检测到 7 轮 FINAL 文件（多次断线重跑），但代码问题一致，数据价值有限。

---

## 第八轮实验 (2026-02-18, Round 5 in code)

### 修复内容

1. 训练服务器用 `git reset --hard origin/main` 强制覆盖代码
2. `restart_all.sh` 增加代码验证检查，缺失关键修复则中止

### 结果

36/36 全部完成，约 561 分钟。

代码修复确认生效：
- ✅ step_reward 字段存在
- ✅ reward 全部非零 (1951/1951)
- ✅ DR 有 solver_reward 数据

### 新发现的 Bug: warmup 永远不结束

PAIRED 和 AT-PC 全程 2000 iter 都在 WARMUP 阶段，从未进入 GENERATOR。

原因：`warmup_count` 只在 `generate_and_apply()` 被调用时 +1，而该函数每 `curriculum_freq=50` iter 才调用一次。warmup=500 需要 500 次调用 = 500×50 = 25000 iter，但总共只有 2000 iter（40 次调用）。

影响：AT-PC 和 PAIRED 的生成器从未被使用，三种方法实质上都是随机地形（DR），无法体现差异。

solver_reward 趋势（G1-seed42）：
| 方法 | iter 0 | iter 500 | iter 1000 | iter 1500 | iter 2000 |
|------|--------|----------|-----------|-----------|-----------|
| DR | 0.0009 | 0.0059 | 0.0097 | 0.0119 | 0.0116 |
| PAIRED | 0.0009 | 0.0064 | 0.0103 | 0.0120 | 0.0129 |
| AT-PC | 0.0009 | 0.0059 | 0.0102 | 0.0124 | 0.0130 |

三种方法几乎一致，符合预期（都在 warmup = 随机地形）。

### 修复

将 warmup 判断从"调用次数"改为"当前 iteration 数"：
```python
# 旧: warmup_count 每次调用+1，需要500次调用
# 新: 直接比较 current_iter < warmup_iters (500)
def generate_and_apply(self, current_iter=0):
    if not self.warmup_done and current_iter < self.warmup_iters:
        ...
```

这样 warmup 在 iter 500 准时结束，之后 30 次 curriculum 调用使用 GENERATOR。

---

## 第九轮实验 (待运行)

### 待修复确认

- warmup 基于 iter 数（iter < 500 = warmup）
- warmup 结束后应看到 src=GENERATOR/EASY
- AT-PC 的 accept_rate 应 > 0
- 三种方法应出现分化
